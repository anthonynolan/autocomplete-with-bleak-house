{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï»¿The Project Gutenberg eBook, Bleak House, by Charles Dickens\n",
      "\n",
      "\n",
      "This eBook is for the use of anyone \n"
     ]
    }
   ],
   "source": [
    "with open('bleak.txt', 'rt') as f:\n",
    "    content = f.read()\n",
    "print(content[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\ufeffThe', 'Project', 'Gutenberg', 'eBook', 'Bleak', 'House', 'by', 'Charles', 'Dickens', 'This']\n",
      "['<UNK>', 'Project', 'Gutenberg', '<UNK>', 'Bleak', 'House', 'by', '<UNK>', '<UNK>', 'This']\n",
      "[['<UNK>', 'Project', '<UNK>', 'Bleak', 'Gutenberg'], ['Project', 'Gutenberg', 'Bleak', 'House', '<UNK>'], ['Gutenberg', '<UNK>', 'House', 'by', 'Bleak'], ['<UNK>', 'Bleak', 'by', '<UNK>', 'House'], ['Bleak', 'House', '<UNK>', '<UNK>', 'by']]\n",
      "356557 356561\n",
      "2001\n"
     ]
    }
   ],
   "source": [
    "content = content.translate(str.maketrans('', '', string.punctuation))\n",
    "words = word_tokenize(content)\n",
    "print(words[:10])\n",
    "\n",
    "most_common_count = 2000\n",
    "c = Counter(words)\n",
    "most_common = [pair[0] for pair in c.most_common(most_common_count)]\n",
    "words_with_unk = [word if word in most_common else '<UNK>' for word in words]\n",
    "print(words_with_unk[:10])\n",
    "\n",
    "context_size = 2\n",
    "input = []\n",
    "for a in range(context_size, len(words_with_unk)-context_size):\n",
    "    x = [item for sublist in [words_with_unk[a-context_size:a], words_with_unk[a+1:a+context_size+1], [words_with_unk[a]]] for item in sublist]\n",
    "    input.append(x)\n",
    "    \n",
    "print(input[:5])\n",
    "print(len(input), len(words_with_unk))\n",
    "\n",
    "vocab = set(words_with_unk)\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('however', 0), ('At', 1), ('conclusion', 2), ('figure', 3), ('ironmaster', 4)]\n",
      "[('however', array([1., 0., 0., ..., 0., 0., 0.])), ('At', array([0., 1., 0., ..., 0., 0., 0.])), ('conclusion', array([0., 0., 1., ..., 0., 0., 0.])), ('figure', array([0., 0., 0., ..., 0., 0., 0.])), ('ironmaster', array([0., 0., 0., ..., 0., 0., 0.]))]\n"
     ]
    }
   ],
   "source": [
    "index_to_word = {k:v for k, v in enumerate(vocab)}\n",
    "word_to_index = {v:k for k, v in index_to_word.items()}; \n",
    "gen = (item for item in word_to_index.items())\n",
    "print([next(gen) for _ in range(5)])\n",
    "\n",
    "\n",
    "def word_to_vec(word):\n",
    "    vec = np.zeros(len(vocab))\n",
    "    if word in word_to_index.keys():\n",
    "        vec[word_to_index[word]] = 1\n",
    "    else:\n",
    "        vec[word_to_index['<UNK>']] = 1\n",
    "    return vec\n",
    "word_to_one_hot = {}\n",
    "for word in vocab:\n",
    "    word_to_one_hot[word] = word_to_vec(word)\n",
    "    \n",
    "print(list(word_to_one_hot.items())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2001) (1000, 2001)\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "Xs = []\n",
    "Ys = []\n",
    "for row in input[:1000]:\n",
    "    x = np.vstack([word_to_one_hot[word] for word in row[:-1]]).mean(axis=0)\n",
    "    Xs.append(x)\n",
    "    y = [word_to_one_hot[word] for word in row[-1:]]\n",
    "    Ys.append(y)\n",
    "    \n",
    "X = np.vstack(Xs)\n",
    "Y = np.vstack(Ys)\n",
    "\n",
    "print(X.shape, Y.shape)\n",
    "\n",
    "np.set_printoptions(threshold=1002)\n",
    "print(np.argmax(Y[0]))\n",
    "\n",
    "X = csr_matrix(X)\n",
    "Y = csr_matrix(Y)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "layer-1 (Dense)              (None, 2000)              4004000   \n",
      "_________________________________________________________________\n",
      "layer-2 (Dense)              (None, 1000)              2001000   \n",
      "_________________________________________________________________\n",
      "output-layer (Dense)         (None, 2001)              2003001   \n",
      "=================================================================\n",
      "Total params: 8,008,001\n",
      "Trainable params: 8,008,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "\n",
    "model.add(layers.Dense(2000, activation='relu', name='layer-1', input_shape=(X_train.shape[1],)))\n",
    "model.add(layers.Dense(1000, activation='relu', name='layer-2'))\n",
    "model.add(layers.Dense(len(vocab), activation='softmax', name='output-layer'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 750 samples\n",
      "Epoch 1/50\n",
      "750/750 [==============================] - 1s 826us/sample - loss: 7.2551 - categorical_accuracy: 0.2320\n",
      "Epoch 2/50\n",
      "750/750 [==============================] - 0s 336us/sample - loss: 4.6992 - categorical_accuracy: 0.3360\n",
      "Epoch 3/50\n",
      "750/750 [==============================] - 0s 357us/sample - loss: 3.9662 - categorical_accuracy: 0.3360\n",
      "Epoch 4/50\n",
      "750/750 [==============================] - 0s 383us/sample - loss: 3.8228 - categorical_accuracy: 0.3360\n",
      "Epoch 5/50\n",
      "750/750 [==============================] - 0s 399us/sample - loss: 3.7603 - categorical_accuracy: 0.3360\n",
      "Epoch 6/50\n",
      "750/750 [==============================] - 0s 378us/sample - loss: 3.7231 - categorical_accuracy: 0.3360\n",
      "Epoch 7/50\n",
      "750/750 [==============================] - 0s 350us/sample - loss: 3.7058 - categorical_accuracy: 0.3360\n",
      "Epoch 8/50\n",
      "750/750 [==============================] - 0s 341us/sample - loss: 3.6352 - categorical_accuracy: 0.3360\n",
      "Epoch 9/50\n",
      "750/750 [==============================] - 0s 354us/sample - loss: 3.5873 - categorical_accuracy: 0.3360\n",
      "Epoch 10/50\n",
      "750/750 [==============================] - 0s 341us/sample - loss: 3.5507 - categorical_accuracy: 0.3360\n",
      "Epoch 11/50\n",
      "750/750 [==============================] - 0s 348us/sample - loss: 3.5328 - categorical_accuracy: 0.3387\n",
      "Epoch 12/50\n",
      "750/750 [==============================] - 0s 343us/sample - loss: 3.5274 - categorical_accuracy: 0.3427\n",
      "Epoch 13/50\n",
      "750/750 [==============================] - 0s 349us/sample - loss: 3.4329 - categorical_accuracy: 0.3413\n",
      "Epoch 14/50\n",
      "750/750 [==============================] - 0s 343us/sample - loss: 3.3837 - categorical_accuracy: 0.3440\n",
      "Epoch 15/50\n",
      "750/750 [==============================] - 0s 363us/sample - loss: 3.3413 - categorical_accuracy: 0.3493\n",
      "Epoch 16/50\n",
      "750/750 [==============================] - 0s 359us/sample - loss: 3.2863 - categorical_accuracy: 0.3480\n",
      "Epoch 17/50\n",
      "750/750 [==============================] - 0s 352us/sample - loss: 3.2763 - categorical_accuracy: 0.3600\n",
      "Epoch 18/50\n",
      "750/750 [==============================] - 0s 343us/sample - loss: 3.1714 - categorical_accuracy: 0.3680\n",
      "Epoch 19/50\n",
      "750/750 [==============================] - 0s 344us/sample - loss: 3.1166 - categorical_accuracy: 0.3667\n",
      "Epoch 20/50\n",
      "750/750 [==============================] - 0s 354us/sample - loss: 3.0677 - categorical_accuracy: 0.3827\n",
      "Epoch 21/50\n",
      "750/750 [==============================] - 0s 353us/sample - loss: 3.0467 - categorical_accuracy: 0.3720\n",
      "Epoch 22/50\n",
      "750/750 [==============================] - 0s 351us/sample - loss: 2.9648 - categorical_accuracy: 0.3947\n",
      "Epoch 23/50\n",
      "750/750 [==============================] - 0s 349us/sample - loss: 2.9245 - categorical_accuracy: 0.3907\n",
      "Epoch 24/50\n",
      "750/750 [==============================] - 0s 348us/sample - loss: 2.8510 - categorical_accuracy: 0.4000\n",
      "Epoch 25/50\n",
      "750/750 [==============================] - 0s 360us/sample - loss: 2.7586 - categorical_accuracy: 0.4053\n",
      "Epoch 26/50\n",
      "750/750 [==============================] - 0s 351us/sample - loss: 2.6929 - categorical_accuracy: 0.4200\n",
      "Epoch 27/50\n",
      "750/750 [==============================] - 0s 339us/sample - loss: 2.6436 - categorical_accuracy: 0.4200\n",
      "Epoch 28/50\n",
      "750/750 [==============================] - 0s 344us/sample - loss: 2.5849 - categorical_accuracy: 0.4480\n",
      "Epoch 29/50\n",
      "750/750 [==============================] - 0s 359us/sample - loss: 2.4915 - categorical_accuracy: 0.4400\n",
      "Epoch 30/50\n",
      "750/750 [==============================] - 0s 362us/sample - loss: 2.4377 - categorical_accuracy: 0.4573\n",
      "Epoch 31/50\n",
      "750/750 [==============================] - 0s 353us/sample - loss: 2.3318 - categorical_accuracy: 0.4560\n",
      "Epoch 32/50\n",
      "750/750 [==============================] - 0s 342us/sample - loss: 2.2700 - categorical_accuracy: 0.4800\n",
      "Epoch 33/50\n",
      "750/750 [==============================] - 0s 341us/sample - loss: 2.2155 - categorical_accuracy: 0.4787\n",
      "Epoch 34/50\n",
      "750/750 [==============================] - 0s 342us/sample - loss: 2.1191 - categorical_accuracy: 0.5040\n",
      "Epoch 35/50\n",
      "750/750 [==============================] - 0s 350us/sample - loss: 2.0428 - categorical_accuracy: 0.5120\n",
      "Epoch 36/50\n",
      "750/750 [==============================] - 0s 350us/sample - loss: 1.9621 - categorical_accuracy: 0.5293\n",
      "Epoch 37/50\n",
      "750/750 [==============================] - 0s 387us/sample - loss: 1.8562 - categorical_accuracy: 0.5440\n",
      "Epoch 38/50\n",
      "750/750 [==============================] - 0s 382us/sample - loss: 1.8013 - categorical_accuracy: 0.5600\n",
      "Epoch 39/50\n",
      "750/750 [==============================] - 0s 345us/sample - loss: 1.6971 - categorical_accuracy: 0.5813\n",
      "Epoch 40/50\n",
      "750/750 [==============================] - 0s 343us/sample - loss: 1.6704 - categorical_accuracy: 0.6000\n",
      "Epoch 41/50\n",
      "750/750 [==============================] - 0s 362us/sample - loss: 1.6579 - categorical_accuracy: 0.5507\n",
      "Epoch 42/50\n",
      "750/750 [==============================] - 0s 353us/sample - loss: 1.4773 - categorical_accuracy: 0.6373\n",
      "Epoch 43/50\n",
      "750/750 [==============================] - 0s 344us/sample - loss: 1.4111 - categorical_accuracy: 0.6453\n",
      "Epoch 44/50\n",
      "750/750 [==============================] - 0s 348us/sample - loss: 1.3297 - categorical_accuracy: 0.6640\n",
      "Epoch 45/50\n",
      "750/750 [==============================] - 0s 353us/sample - loss: 1.2867 - categorical_accuracy: 0.6787\n",
      "Epoch 46/50\n",
      "750/750 [==============================] - 0s 354us/sample - loss: 1.2531 - categorical_accuracy: 0.6893\n",
      "Epoch 47/50\n",
      "750/750 [==============================] - 0s 349us/sample - loss: 1.1356 - categorical_accuracy: 0.7200\n",
      "Epoch 48/50\n",
      "750/750 [==============================] - 0s 343us/sample - loss: 1.0722 - categorical_accuracy: 0.7387\n",
      "Epoch 49/50\n",
      "750/750 [==============================] - 0s 344us/sample - loss: 1.0076 - categorical_accuracy: 0.7467\n",
      "Epoch 50/50\n",
      "750/750 [==============================] - 0s 340us/sample - loss: 1.0852 - categorical_accuracy: 0.7307\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x154232a50>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(),  \n",
    "    loss=keras.losses.CategoricalCrossentropy(),\n",
    "    metrics=[keras.metrics.CategoricalAccuracy()],\n",
    ")\n",
    "model.fit(X_train.toarray(), Y_train.toarray(), batch_size=256, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/first_model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('saved_model/first_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "layer-1 (Dense)              (None, 2000)              4004000   \n",
      "_________________________________________________________________\n",
      "layer-2 (Dense)              (None, 1000)              2001000   \n",
      "_________________________________________________________________\n",
      "output-layer (Dense)         (None, 2001)              2003001   \n",
      "=================================================================\n",
      "Total params: 8,008,001\n",
      "Trainable params: 8,008,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model = tf.keras.models.load_model('saved_model/first_model')\n",
    "model.build(input_shape=(x_train.shape[1],))\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
